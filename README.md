# TweetEval
This is the main repository of the TweetEval benchmark (reference paper here)

# TweetEval: Datasets

TODO: Explain tasks and dataset formats + add links to papers


- **Emotion Recognition**, SemEval 2018 (Affects in Tweets)

- **Emoji Prediction**, SemEval 2018 (Emoji Prediction)

- **Irony Detection**, SemEval 2018 (Irony Detection)

- **Hate Speech Detection**, SemEval 2019 (Hateval)

- **Offensive Language Identification**, SemEval 2019 (OffensEval)

- **Sentiment Analysis**, SemEval 2017 (Sentiment Analysis in Twitter)

- **Stance Detection**, SemEval 2016 (Detecting Stance in Tweets)

# TweetEval: Leaderboard

TODO: Add table with leaderboard



# Evaluating your system

For evaluating your system, you simply need a prediction file with the same format as the output example XXX. This is, you would need a prediction file for each task......

# Citing TweetEval

If you use TweetEval in your research, please use the following `bib` entry.

```
@inproceedings{barbieri2020tweeteval,
  title={{TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification}},
  author={Barbieri, Francesco and Camacho-Collados, Jose and Espinosa-Anke, Luis and Neves, Leonardo},
  booktitle={Proceedings of Findings of EMNLP},
  year={2020}
}
```
# License

TweetEval is released without any restrictions but restrictions may apply to individual tasks (which are derived from existing datasets) or Twitter (main data source). We refer users to the original licenses accompanying each dataset and Twitter regulations.


# Citing TweetEval datasets

If you use any of the TweetEval datasets, please cite their original publications:

TODO: Add bibfiles for each task
